{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sazsa6/Learning-Deep-Learning/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b85b17f-5b53-4ee2-9e01-1a6e5250758c",
      "metadata": {
        "id": "3b85b17f-5b53-4ee2-9e01-1a6e5250758c"
      },
      "source": [
        "## Phi-3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcab8efd-4224-4c1c-be36-354b3398337c",
      "metadata": {
        "id": "fcab8efd-4224-4c1c-be36-354b3398337c"
      },
      "source": [
        "# 1. Setup: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8022df-dc66-452f-b977-6487b60afaed",
      "metadata": {
        "id": "ac8022df-dc66-452f-b977-6487b60afaed"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6677b8ea-cefc-403b-9730-58cc42022221",
      "metadata": {
        "id": "6677b8ea-cefc-403b-9730-58cc42022221"
      },
      "source": [
        "# 2. Manual Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420dbe4c-7c47-4a95-a17a-c63afe6fea24",
      "metadata": {
        "id": "420dbe4c-7c47-4a95-a17a-c63afe6fea24"
      },
      "source": [
        "## 2.1. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b458554-de6c-432e-8942-6fab60224ccb",
      "metadata": {
        "id": "1b458554-de6c-432e-8942-6fab60224ccb"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# Load the causal language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Load the tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d53c72-55be-4917-8d44-d60cdeeaf5fd",
      "metadata": {
        "id": "96d53c72-55be-4917-8d44-d60cdeeaf5fd"
      },
      "source": [
        "## 2.2. Tokenization Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fce2f45-9910-4510-aec9-75b2219f4401",
      "metadata": {
        "id": "9fce2f45-9910-4510-aec9-75b2219f4401"
      },
      "outputs": [],
      "source": [
        "# Define the text variable\n",
        "text = \"This is some text to tokenize.\"\n",
        "\n",
        "# Encode text into input IDs\n",
        "encoding = tokenizer(text)\n",
        "\n",
        "# Convert input IDs to readable tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
        "\n",
        "# Print token IDs and their corresponding tokens\n",
        "for token_id, token_str in zip(encoding['input_ids'], tokens):\n",
        "    print(f\"{token_id} --> {token_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3576b2f-23f1-426d-8e62-14a36bf93494",
      "metadata": {
        "id": "c3576b2f-23f1-426d-8e62-14a36bf93494"
      },
      "source": [
        "## 2.3. Model Inference (Logits and Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a344a1-04c5-4520-847d-b7da301e70c3",
      "metadata": {
        "id": "e0a344a1-04c5-4520-847d-b7da301e70c3"
      },
      "outputs": [],
      "source": [
        "# Tokenize and move input to same device as model\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Get raw output logits from model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Print max logit values (confidence per token position)\n",
        "print(outputs.logits[0][0].max())  # For first token\n",
        "print(outputs.logits[0][1].max())  # For second token\n",
        "\n",
        "# Print predicted token IDs (most likely next token at each position)\n",
        "print(outputs.logits[0][0].argmax())  # ID of most likely first token\n",
        "print(outputs.logits[0][1].argmax())  # ID of most likely second token\n",
        "\n",
        "# Decode a couple of token IDs manually\n",
        "print(tokenizer.decode([29892, 29991]))\n",
        "print(tokenizer.decode(outputs.logits[0].argmax(-1)))\n",
        "\n",
        "# Full predicted token sequence from logits\n",
        "predicted_token_id = outputs.logits.argmax(dim=-1)\n",
        "print(tokenizer.decode(predicted_token_id[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5eff03-2bb8-4d15-9288-9d24436f23e3",
      "metadata": {
        "id": "ac5eff03-2bb8-4d15-9288-9d24436f23e3"
      },
      "source": [
        "## 2.4. Generate continuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "525fb8d7-8d13-43dc-93f0-db6fe5e2aefb",
      "metadata": {
        "id": "525fb8d7-8d13-43dc-93f0-db6fe5e2aefb"
      },
      "outputs": [],
      "source": [
        "# Generate continuation\n",
        "generated_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    #max_new_tokens=50,       # Number of tokens to generate\n",
        "    do_sample=True,         # Use greedy decoding (set True for randomness)\n",
        "    pad_token_id=tokenizer.eos_token_id  # Avoid warning if eos is not set\n",
        ")\n",
        "\n",
        "# Decode and print result\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14017ff5-6267-4648-8352-0e5918ebaeec",
      "metadata": {
        "id": "14017ff5-6267-4648-8352-0e5918ebaeec"
      },
      "source": [
        "# 3. Pipeline Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3192d2b-3669-4c7b-b5a2-2318a8895fad",
      "metadata": {
        "id": "b3192d2b-3669-4c7b-b5a2-2318a8895fad"
      },
      "source": [
        "## 3.1. Pipeline Preparation"
      ]
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4FSiHpJY73k_"
      },
      "id": "4FSiHpJY73k_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec88d546-0390-41e4-9af6-e8d271a73899",
      "metadata": {
        "id": "ec88d546-0390-41e4-9af6-e8d271a73899"
      },
      "outputs": [],
      "source": [
        "# Create a text-generation pipeline using the same model and tokenizer\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,   # Limit length of generated text\n",
        "    do_sample=False       # Deterministic (no sampling)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ff5493-86d9-4785-b703-89432b810d3e",
      "metadata": {
        "id": "d2ff5493-86d9-4785-b703-89432b810d3e"
      },
      "source": [
        "## 3.2. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64c4ebd-b22b-49d4-9668-36c7b6d5f168",
      "metadata": {
        "id": "a64c4ebd-b22b-49d4-9668-36c7b6d5f168"
      },
      "outputs": [],
      "source": [
        "# Prompt for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello World\"}\n",
        "]\n",
        "\n",
        "# Generate text using the prompt\n",
        "output = generator(messages)\n",
        "\n",
        "# Print generated output\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22261cc-abf1-47a0-9835-359b50356469",
      "metadata": {
        "id": "b22261cc-abf1-47a0-9835-359b50356469"
      },
      "outputs": [],
      "source": [
        "# The prompt (user input / query)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get image for this chatbot\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Option 1: Display an image from Google Drive\n",
        "# Make sure you have mounted your Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# display(Image('/content/drive/My Drive/chatbot_image.png'))\n",
        "\n",
        "# Option 2: Display an image from a URL\n",
        "# Replace with the actual URL of the image\n",
        "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/ChatGPT_logo.svg/200px-ChatGPT_logo.svg.png'\n",
        "display(Image(url=image_url))\n"
      ],
      "metadata": {
        "id": "YBIrTn3yCCxD"
      },
      "id": "YBIrTn3yCCxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9e680e95-d758-4bdb-9f11-1075130a38d6",
      "metadata": {
        "id": "9e680e95-d758-4bdb-9f11-1075130a38d6"
      },
      "source": [
        "## 3.3. Small Chatbot! Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d670867-6a00-41f3-b04d-1fecdb0ad5a5",
      "metadata": {
        "id": "0d670867-6a00-41f3-b04d-1fecdb0ad5a5"
      },
      "outputs": [],
      "source": [
        "# Store chat history\n",
        "messages = []\n",
        "\n",
        "print(\"Chatbot is ready! Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print('Conversation has ended!')\n",
        "        break\n",
        "\n",
        "    # Add user message to history\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Generate model response\n",
        "    response = generator(messages)\n",
        "\n",
        "    # Get the assistant reply\n",
        "    reply = response[0][\"generated_text\"]\n",
        "    print(\"Bot:\", reply)\n",
        "\n",
        "    # Add assistant reply to history\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "eocv",
      "language": "python",
      "name": "eocv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}